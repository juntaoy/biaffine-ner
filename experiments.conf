# Word embeddings.
ft_en_300d {
  path = ../fasttext/cc.en.300.vec.filtered
  size = 300
}
ft_de_300d {
  path = ../fasttext/cc.de.300.vec.filtered
  size = 300
}
ft_nl_300d {
  path = ../fasttext/cc.nl.300.vec.filtered
  size = 300
}
ft_es_300d {
  path = ../fasttext/cc.es.300.vec.filtered
  size = 300
}
w2v_cop_50d{
  path = vec/coptic_50d.vec
  size = 50
}

base  {
  ffnn_size = 150
  ffnn_depth = 2
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = ""
  context_embeddings = ${ft_en_300d}
  contextualization_size = 200
  contextualization_layers = 3
  lm_size = 1024
  lm_layers = 4
  lm_path = ""

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lstm_dropout_rate = 0.4
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = ""
  eval_path = ""
  lm_path = ""
  test_path = ""
  ner_types = []
  eval_frequency = 500
  report_frequency = 100
  log_root = logs
  max_step = 40000
}


# Main configuration.
eng_genia = ${base}{
  train_path = train_dev.genia.jsonlines
  test_path = test.genia.jsonlines
  lm_path = bert-model/bert_genia_features.hdf5
  ner_types = ["DNA","RNA","protein","cell_line","cell_type"]
  char_vocab_path = "char_vocab.eng.genia.txt"
  max_step = 80000
}


eng_ace04 = ${base}{
  train_path = train.ACE2004.jsonlines
  eval_path = dev.ACE2004.jsonlines
  lm_path = bert-model/bert_ace2004_features.hdf5
  test_path = test.ACE2004.jsonlines
  ner_types = ["LOC","WEA","GPE","PER","FAC","ORG","VEH"]
  char_vocab_path = "char_vocab.eng.ace2004.txt"
  max_step = 40000
}

eng_ace05 = ${base}{
  train_path = train.ACE2005.jsonlines
  eval_path = dev.ACE2005.jsonlines
  lm_path = bert-model/bert_ace2005_features.hdf5
  test_path = test.ACE2005.jsonlines
  ner_types = ["LOC","WEA","GPE","PER","FAC","ORG","VEH"]
  char_vocab_path = "char_vocab.eng.ace2005.txt"
  max_step = 100000
}

eng_conll12 = ${base}{
  train_path = train.conll12.jsonlines
  eval_path = dev.conll12.jsonlines
  lm_path = bert-model/bert_conll2012_features.hdf5
  test_path = test.conll12.jsonlines
  ner_types = ["ORDINAL","LOC","PRODUCT","NORP","WORK_OF_ART","LANGUAGE","GPE","TIME","PERCENT","MONEY","PERSON","CARDINAL","FAC","DATE","ORG","LAW","EVENT","QUANTITY"]
  char_vocab_path = "char_vocab.eng.conll12.txt"
  flat_ner = true
  max_step = 200000
}

eng_conll03 = ${base}{
  train_path = train_dev.conll03.jsonlines
  lm_path = bert-model/bert_conll03_features.hdf5
  test_path = test.conll03.jsonlines
  ner_types = ["ORG","MISC","PER","LOC"]
  char_vocab_path = "char_vocab.eng.conll03.txt"
  flat_ner = true
  max_step = 80000
}

deu_conll03 = ${base}{
  train_path = train.deu.conll03.corrected.jsonlines
  lm_path = bert-model/bert_deu_conll03_features.hdf5
  test_path = test.deu.conll03.corrected.jsonlines
  ner_types = ["ORG","MISC","PER","LOC"]
  char_vocab_path = "char_vocab.deu.conll03.txt"
  context_embeddings = ${ft_de_300d}
  lm_size = 768
  flat_ner = true
  max_step = 100000
}

deu_conll03_revised = ${deu_conll03}{
  train_path = train_dev.deu.conll03.revised06.jsonlines
  test_path = test.deu.conll03.revised06.jsonlines
}

esp_conll02 = ${base}{
  train_path = train_dev.esp.conll02.jsonlines
  lm_path = bert-model/bert_esp_conll02_features.hdf5
  test_path = test.esp.conll02.jsonlines
  ner_types = ["ORG","MISC","PER","LOC"]
  char_vocab_path = "char_vocab.esp.conll02.txt"
  context_embeddings = ${ft_es_300d}
  lm_size = 768
  flat_ner = true
}

ned_conll02 = ${base}{
  train_path = train_dev.ned.conll02.jsonlines
  lm_path = bert-model/bert_ned_conll02_features.hdf5
  test_path = test.ned.conll02.jsonlines
  ner_types = ["ORG","MISC","PER","LOC"]
  char_vocab_path = "char_vocab.ned.conll02.txt"
  context_embeddings = ${ft_nl_300d}
  lm_size = 768
  flat_ner = true
}


# Parameters for experiment using UD Coptic data
cop = ${base}{
  train_path = cop_train.jsonlines
  lm_path = none
  eval_path = cop_dev.jsonlines
  test_path = cop_test.jsonlines
  ner_types = ["thing"]
  # comment out prev line and uncomment next to do 10-class classification instead of binary entity recognition
  #ner_types = ["abstract","animal","object","event","place","organization","time","substance","plant","person"]
  char_vocab_path = "char_vocab.cop.txt"
  context_embeddings = ${w2v_cop_50d}
  lm_size = 1
  lm_layers = 1
  flat_ner = false
  contextualization_size = 40
  contextualization_layers = 1

  eval_frequency = 500
  report_frequency = 200
  log_root = logs
  max_step = 8000
  
  lstm_dropout_rate = 0.1
  lexical_dropout_rate = 0.1
  dropout_rate = 0.1
  learning_rate = 0.001
  ffnn_size = 50
  ffnn_depth = 2
  char_embedding_size = 8

}